{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç”¨VGG13å®ç°cifaræ•°æ®é›†ä¸Šçš„å›¾åƒè¯†åˆ«\n",
    "\n",
    "## The CIFAR-10 dataset\n",
    "\n",
    "**ç½‘å€**ï¼š\n",
    "\n",
    "http://www.cs.toronto.edu/~kriz/cifar.html?usg=alkjrhjqbhw2llxlo8emqns-tbk0at96jq\n",
    "\n",
    "**ä»‹ç»**ï¼š\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "<img src=\"CIFAR-10.PNG\" width=\"50%\">\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
    "\n",
    "åœ¨TensorFlowä¸­ï¼Œé€šè¿‡datasets.cifar10.load_data()å‡½æ•°å°±å¯ä»¥ç›´æ¥åŠ è½½åˆ‡å‰²å¥½çš„è®­ç»ƒé›†å’Œæ•°æ®é›†ã€‚\n",
    "\n",
    "TensorFlowä¼šè‡ªåŠ¨å°†æ•°æ®é›†ä¸‹è½½åœ¨ `C:\\Users\\ç”¨æˆ·å\\.keras\\datasets` è·¯å¾„ä¸‹ï¼Œç”¨æˆ·å¯ä»¥æŸ¥çœ‹ï¼Œä¹Ÿå¯æ‰‹åŠ¨åˆ é™¤ä¸éœ€è¦çš„æ•°æ®é›†ç¼“å­˜ã€‚ä¸Šè¿°ä»£ç è¿è¡Œåï¼Œå¾—åˆ°è®­ç»ƒé›†çš„**x**å’Œ**y**å½¢çŠ¶ä¸ºï¼š (50000, 32, 32, 3)å’Œ(50000)ï¼Œæµ‹è¯•é›†çš„**x**å’Œ**ğ’š**å½¢çŠ¶ä¸º(10000, 32, 32, 3)å’Œ(10000)ï¼Œåˆ†åˆ«ä»£è¡¨äº† å›¾ç‰‡å¤§å°ä¸º32 Ã— 32ï¼Œå½©è‰²å›¾ç‰‡ï¼Œè®­ç»ƒé›†æ ·æœ¬æ•°ä¸º 50000ï¼Œæµ‹è¯•é›†æ ·æœ¬æ•°ä¸º 10000ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:45:59.615747Z",
     "start_time": "2020-02-21T03:45:42.238896Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Sequential,optimizers,losses,datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x,y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "#åˆ é™¤yçš„ä¸€ä¸ªç»´åº¦,[b,1]=>[b]\n",
    "y=tf.squeeze(y,axis=1)\n",
    "y_test=tf.squeeze(y_test,axis=1)\n",
    "#æ‰“å°è®­ç»ƒæ¥å’Œæµ‹è¯•é›†çš„å½¢çŠ¶\n",
    "print(x.shape,y.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # å°†æ•°æ®æ˜ å°„åˆ°-1~1\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32) # ç±»å‹è½¬æ¢\n",
    "    return x,y\n",
    "\n",
    "#æ„å»ºè®­ç»ƒé›†å¯¹è±¡ï¼Œéšæœºæ‰“ä¹±ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–\n",
    "train_db=tf.data.Dataset.from_tensor_slices((x,y))\n",
    "train_db=train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "#æ„å»ºæµ‹è¯•é›†å¯¹è±¡ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–\n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "test_db=test_db.map(preprocess).batch(128)\n",
    "#ä»è®­ç»ƒé›†ä¸­é‡‡ç”¨ä¸€ä¸ªBatchï¼Œå¹¶è§‚å¯Ÿ\n",
    "sample=next(iter(train_db))\n",
    "print('sample:',sample[0].shape,sample[1].shape,tf.reduce_min(sample[0]),tf.reduce_max(sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG13\n",
    "æˆ‘ä»¬å°†åŸºäºVGG13ç½‘ç»œï¼Œæ ¹æ®æˆ‘ä»¬çš„æ•°æ®é›†ç‰¹ç‚¹ä¿®æ”¹éƒ¨åˆ†ç½‘ç»œç»“æ„ï¼Œå®Œæˆ CIFAR10 å›¾ç‰‡è¯†åˆ«ï¼Œä¿®æ”¹å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. å°†ç½‘ç»œè¾“å…¥è°ƒæ•´ä¸º32Ã—32ã€‚åŸç½‘ç»œè¾“å…¥ä¸º22Ã—22ï¼Œå¯¼è‡´å…¨è¿æ¥å±‚è¾“å…¥ç‰¹å¾ç»´åº¦è¿‡å¤§ï¼Œç½‘ç»œå‚æ•°é‡è¿‡å¤§ã€‚\n",
    "\n",
    "2. 3ä¸ªå…¨è¿æ¥å±‚çš„ç»´åº¦è°ƒæ•´ä¸º[256,64,10]ï¼Œæ»¡è¶³10åˆ†ç±»ä»»åŠ¡çš„è®¾å®šã€‚\n",
    "\n",
    "<img src=\"VGG13.PNG\">\n",
    "\n",
    "æˆ‘ä»¬å°†ç½‘ç»œå®ç°ä¸º 2ä¸ªå­ç½‘ç»œï¼š**å·ç§¯å­ç½‘ç»œ**å’Œ**å…¨è¿æ¥å­ç½‘ç»œ**ã€‚å·ç§¯å­ç½‘ç»œç”±5ä¸ªå­æ¨¡å—æ„æˆï¼Œæ¯ä¸ªå­æ¨¡å—åŒ…å«äº†**Conv-Conv-MaxPooling**å•å…ƒç»“æ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T11:38:18.868046Z",
     "start_time": "2020-02-20T11:38:18.577822Z"
    }
   },
   "outputs": [],
   "source": [
    "#å·ç§¯å­ç½‘ç»œ\n",
    "conv_layers=[\n",
    "    #å…ˆåˆ›å»ºåŒ…å«å¤šç½‘ç»œçš„ç±»åˆ«\n",
    "    #Conv-Conv-Poolingå•å…ƒ1\n",
    "    #64ä¸ª3*3çš„å·ç§¯æ ¸ï¼Œè¾“å…¥è¾“å‡ºåŒå¤§å°\n",
    "    layers.Conv2D(64,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.Conv2D(64,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    #é«˜å®½å‡åŠ\n",
    "    layers.MaxPooling2D(pool_size=[2,2],strides=2,padding=\"same\"),\n",
    "    \n",
    "    #Conv-Conv-Pooling å•å…ƒ 2,è¾“å‡ºé€šé“æå‡è‡³ 128ï¼Œé«˜å®½å¤§å°å‡åŠ\n",
    "    layers.Conv2D(128,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.Conv2D(128,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D(pool_size=[2,2],strides=2,padding=\"same\"),\n",
    "    \n",
    "    # Conv-Conv-Pooling å•å…ƒ 3,è¾“å‡ºé€šé“æå‡è‡³ 256ï¼Œé«˜å®½å¤§å°å‡åŠ\n",
    "    layers.Conv2D(256,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.Conv2D(256,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D(pool_size=[2,2],strides=2,padding=\"same\"),\n",
    "    \n",
    "    # Conv-Conv-Pooling å•å…ƒ 4,è¾“å‡ºé€šé“æå‡è‡³ 512ï¼Œé«˜å®½å¤§å°å‡åŠ\n",
    "    layers.Conv2D(512,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.Conv2D(512,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D(pool_size=[2,2],strides=2,padding=\"same\"),\n",
    "    \n",
    "    # Conv-Conv-Pooling å•å…ƒ 5,è¾“å‡ºé€šé“æå‡è‡³ 512ï¼Œé«˜å®½å¤§å°å‡åŠ\n",
    "    layers.Conv2D(512,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.Conv2D(512,kernel_size=[3,3],padding=\"same\",activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D(pool_size=[2,2],strides=2,padding=\"same\")\n",
    "    \n",
    "    #æ€è€ƒï¼šä¸ºä»€ä¹ˆè¶Šå¾€åé€šé“æ•°è¶Šå¤š\n",
    "    #å›ç­”ï¼šå›¾ç‰‡æ•°æ®çš„è¯†åˆ«è¿‡ç¨‹ä¸€èˆ¬è®¤ä¸ºä¹Ÿæ˜¯è¡¨ç¤ºå­¦ä¹ (Representation Learning)çš„è¿‡ç¨‹ï¼Œ\n",
    "    #ä»æ¥å—åˆ°çš„åŸå§‹åƒç´ ç‰¹å¾å¼€å§‹ï¼Œé€æ¸æå–è¾¹ç¼˜ã€è§’ç‚¹ç­‰åº•å±‚ç‰¹å¾ï¼Œ\n",
    "    #å†åˆ°çº¹ç†ç­‰ä¸­å±‚ç‰¹å¾ï¼Œ\n",
    "    #å†åˆ°å¤´ éƒ¨ã€ç‰©ä½“éƒ¨ä»¶ç­‰é«˜å±‚ç‰¹å¾ã€‚\n",
    "    #æ‰€ä»¥å‰é¢çš„å·ç§¯å±‚é€šé“å°‘ï¼Œæå–çš„æ˜¯åº•å±‚ç‰¹å¾\n",
    "    #åé¢çš„å·ç§¯å’Œé€šé“å¤šï¼Œæå–çš„æ˜¯é«˜å±‚ç‰¹å¾\n",
    "    \n",
    "    #æ€è€ƒï¼šä¸ºä»€ä¹ˆè¦æ± åŒ–\n",
    "    ]\n",
    "\n",
    "#åˆ©ç”¨å‰é¢åˆ›å»ºçš„å±‚åˆ—è¡¨æ„å»ºç½‘ç»œå®¹å™¨\n",
    "conv_net=Sequential(conv_layers)\n",
    "\n",
    "#å…¨è¿æ¥å­ç½‘ç»œ\n",
    "fc_net=Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),\n",
    "    layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(10,activation=None)])\n",
    "\n",
    "#buildä¸¤ä¸ªå­ç½‘ç»œï¼Œå¹¶æ‰“å°ç½‘ç»œå‚æ•°ä¿¡æ¯\n",
    "conv_net.build(input_shape=(None,32,32,3))\n",
    "fc_net.build(input_shape=(None,512))\n",
    "conv_net.summary()\n",
    "fc_net.summary()\n",
    "\n",
    "#è®¾ç½®å­¦ä¹ ç‡,é»˜è®¤å€¼0.001\n",
    "optimizer=optimizers.Adam(lr=1e-4)\n",
    "#éœ€è¦æ›´æ–°çš„å‚æ•°\n",
    "variables=conv_net.trainable_variables+fc_net.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒå¹¶ä¿å­˜ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T11:38:32.432796Z",
     "start_time": "2020-02-20T11:38:26.724053Z"
    }
   },
   "outputs": [],
   "source": [
    "#æ¨¡å‹è®¡ç®—\n",
    "def main():\n",
    "    for epoch in range(10):\n",
    "        for step,(x,y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                #[b,32,32,3] => [b,1,1,512]\n",
    "                out=conv_net(x)\n",
    "                #flatten => [b,512]\n",
    "                out=tf.reshape(out,[-1,512])\n",
    "                #[b,512] => [b,10]\n",
    "                logits=fc_net(out)\n",
    "                #[b] => [b,10]\n",
    "                y_onehot=tf.one_hot(y,depth=10)\n",
    "                #compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot,logits,from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            grads = tape.gradient(loss,variables)\n",
    "            optimizer.apply_gradients(zip(grads,variables))\n",
    "\n",
    "            if step%100==0:\n",
    "                print(epoch,step,'loss:',float(loss))\n",
    "                \n",
    "        total_num=0\n",
    "        total_corret=0\n",
    "        for x,y in test_db:\n",
    "            out=conv_net(x)\n",
    "            out=tf.reshape(out,[-1,512])\n",
    "            logits=fc_net(out)\n",
    "            prob=tf.nn.softmax(logits,axis=1)\n",
    "            pred=tf.argmax(prob,axis=1)\n",
    "            pred=tf.cast(pred,dtype=tf.int32)\n",
    "            \n",
    "            correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "            correct=tf.reduce_sum(correct)\n",
    "            \n",
    "            total_num+=x.shape[0]\n",
    "            total_corret+=int(correct)\n",
    "        \n",
    "        acc=total_corret/total_num\n",
    "        print(epoch,'acc:',acc)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save and load the model**\n",
    "\n",
    "High level keras `model.save` and `tf.keras.models.load_model`\n",
    "\n",
    "```\n",
    "keras_model_path = \"/tmp/keras_save\"\n",
    "model.save(keras_model_path)  # save() should be called out of strategy scope\n",
    "```\n",
    "\n",
    "Low level `tf.saved_model.save` and `tf.saved_model.load`\n",
    "\n",
    "å³SavedModelæ–¹å¼ï¼ŒTensorFlowä¹‹æ‰€ä»¥èƒ½å¤Ÿè¢«ä¸šç•Œé’çï¼Œé™¤äº†ä¼˜ç§€çš„ç¥ç»ç½‘ç»œå±‚APIæ”¯æŒä¹‹å¤–ï¼Œè¿˜å¾—ç›Šäºå®ƒå¼ºå¤§çš„ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç§»åŠ¨ç«¯å’Œç½‘é¡µç«¯ç­‰çš„æ”¯æŒã€‚å½“éœ€è¦å°†æ¨¡å‹éƒ¨ç½²åˆ°å…¶ä»–å¹³å°æ—¶ï¼Œé‡‡ç”¨ TensorFlowæå‡ºçš„SavedModelæ–¹å¼æ›´å…·æœ‰å¹³å°æ— å…³æ€§ã€‚\n",
    "\n",
    "å‚è€ƒç½‘å€:\n",
    "\n",
    "https://www.tensorflow.org/tutorials/distribute/save_and_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T16:23:05.467303Z",
     "start_time": "2020-02-11T16:23:05.213953Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_net.save('conv_net.h5') \n",
    "print('saving conv-net')\n",
    "#del conv_net#åˆ é™¤ç½‘ç»œå¯¹è±¡\n",
    "fc_net.save('fc_net.h5') \n",
    "print('saving fc_net.')\n",
    "#del fc_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½ç½‘ç»œ\n",
    "```\n",
    "restored_keras_model = tf.keras.models.load_model(keras_model_path)\n",
    "restored_keras_model.fit(train_dataset, epochs=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:46:35.933087Z",
     "start_time": "2020-02-21T03:46:35.472854Z"
    }
   },
   "outputs": [],
   "source": [
    "print('load conv_net from file.')\n",
    "conv_net = keras.models.load_model('conv_net.h5')\n",
    "print('load fc_net from file.')\n",
    "fc_net = keras.models.load_model('fc_net.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…³äºWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
    "            \n",
    "è§ï¼š[userwarning-no-training-configuration-found-in-save-file-the-model-was-not-c](https://stackoverflow.com/questions/53295570/userwarning-no-training-configuration-found-in-save-file-the-model-was-not-c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‰ªè£å¹¶æ˜¾ç¤ºå›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:46:52.234506Z",
     "start_time": "2020-02-21T03:46:51.792024Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#è¯»å–å›¾ç‰‡\n",
    "import imageio\n",
    "image_value = imageio.imread('cat.jpg')\n",
    "plt.imshow(image_value)\n",
    "plt.show()\n",
    "\n",
    "image_value=tf.image.resize(image_value,[32,32],antialias=True)\n",
    "image_value=tf.cast(image_value,tf.int32)\n",
    "plt.imshow(image_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¾“å‡ºå›¾ç‰‡è¯†åˆ«ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:46:57.791859Z",
     "start_time": "2020-02-21T03:46:57.785873Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer=dict({0:\"airplane\",\n",
    "        1:\"automobile\",\n",
    "        2:\"bird\",\n",
    "        3:\"cat\",\n",
    "        4:\"deer\",\n",
    "        5:\"dog\",\n",
    "        6:\"frog\",\n",
    "        7:\"horse\",\n",
    "        8:\"ship\",\n",
    "        9:\"truck\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:46:58.474767Z",
     "start_time": "2020-02-21T03:46:58.463796Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(file_name):\n",
    "    image_value=imageio.imread(file_name)[::,::,0:3]#å› ä¸ºè¯»è¿›æ¥æ—¶æ˜¯å››é€šé“\n",
    "    plt.imshow(image_value)\n",
    "    image_value=tf.image.resize(image_value,[32,32],antialias=True)\n",
    "    image_value=tf.expand_dims(image_value,axis=0)\n",
    "    image_value=2*tf.cast(image_value, dtype=tf.float32) / 255. - 1\n",
    "    out=conv_net(image_value)     \n",
    "    out=tf.reshape(out,[-1,512])\n",
    "    logits=fc_net(out)\n",
    "    res=tf.argmax(logits,axis=1)\n",
    "    plt.xlabel(transfer[int(res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:47:04.407035Z",
     "start_time": "2020-02-21T03:46:59.375971Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name=\"cat.jpg\"\n",
    "test(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‹¥æŠ¥é”™ï¼šFailed to get convolution algorithm. This is probably because cuDNN failed to initialize,...\n",
    "\n",
    "å¯èƒ½åŸå› æ˜¯GPUå†…å­˜ä¸è¶³é€ æˆçš„ï¼ˆé‡å¯å†…æ ¸ï¼‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "129px",
    "width": "296px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
