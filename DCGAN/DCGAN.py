# -*- coding: utf-8 -*-
"""
Created on Fri Feb 21 20:25:43 2020

@author: XiaoBanni
"""

import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,Sequential,optimizers,losses,datasets
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.datasets import make_moons 
from sklearn.model_selection import train_test_split
from PIL import Image
import imageio
import  glob

#%%
'''
åŠ è½½æ•°æ®é›†
'''
from dataset import make_anime_dataset

#è¯»å–æ•°æ®é›†è·¯å¾„ï¼Œä»https://pan.baidu.com/s/1eSifHcA æå–ç ï¼šg5qa ä¸‹è½½è§£å‹
img_path=glob.glob(r'faces/*.jpg')
#globæ˜¯pythonè‡ªå·±å¸¦çš„ä¸€ä¸ªæ–‡ä»¶æ“ä½œç›¸å…³æ¨¡å—ï¼Œç”¨å®ƒå¯ä»¥æŸ¥æ‰¾ç¬¦åˆè‡ªå·±ç›®çš„çš„æ–‡ä»¶
#globæ¨¡å—çš„ä¸»è¦æ–¹æ³•å°±æ˜¯glob,è¯¥æ–¹æ³•è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆlistï¼‰ï¼›
#è¯¥æ–¹æ³•éœ€è¦ä¸€ä¸ªå‚æ•°ç”¨æ¥æŒ‡å®šåŒ¹é…çš„è·¯å¾„å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦ä¸²å¯ä»¥ä¸ºç»å¯¹è·¯å¾„ä¹Ÿå¯ä»¥ä¸ºç›¸å¯¹è·¯å¾„ï¼‰ï¼Œå…¶è¿”å›çš„æ–‡ä»¶ååªåŒ…æ‹¬å½“å‰ç›®å½•é‡Œçš„æ–‡ä»¶åï¼Œä¸åŒ…æ‹¬å­æ–‡ä»¶å¤¹é‡Œçš„æ–‡ä»¶ã€‚
batch_size=128
dataset,img_shape,_=make_anime_dataset(img_path, batch_size=batch_size,resize=64)
#å…¶ä¸­datasetå¯¹è±¡å°±æ˜¯tf.data.Datasetç±»å®ä¾‹ï¼Œå·²ç»å®Œæˆäº†éšæœºæ‰“æ•£ã€é¢„å¤„ç†å’Œæ‰¹é‡åŒ–ç­‰æ“ä½œï¼Œå¯ä»¥ç›´æ¥è¿­ä»£è·å¾—æ ·æœ¬æ‰¹ï¼Œimg_shapeæ˜¯é¢„å¤„ç†åçš„å›¾ç‰‡å¤§å°ã€‚
dataset=dataset.repeat(300)

#%%
'''
ç”Ÿæˆå™¨

ç”Ÿæˆç½‘ç»œGç”± 5 ä¸ªè½¬ç½®å·ç§¯å±‚å•å…ƒå †å è€Œæˆï¼Œå®ç°ç‰¹å¾å›¾é«˜å®½çš„å±‚å±‚æ”¾å¤§ï¼Œç‰¹å¾å›¾é€šé“æ•°çš„å±‚å±‚å‡å°‘ã€‚
é¦–å…ˆå°†é•¿åº¦ä¸º100çš„éšè—å‘é‡ğ’›é€šè¿‡Reshapeæ“ä½œè°ƒæ•´ä¸º[ğ‘, 1,1,100]çš„4ç»´å¼ é‡ï¼Œ
å¹¶ä¾åºé€šè¿‡è½¬ç½®å·ç§¯å±‚ï¼Œæ”¾å¤§é«˜å®½ç»´åº¦ï¼Œå‡å°‘é€šé“æ•°ç»´åº¦ï¼Œæœ€åå¾—åˆ°é«˜å®½ä¸º 64ï¼Œé€šé“æ•°ä¸º3çš„å½©è‰²å›¾ç‰‡ã€‚
æ¯ä¸ªå·ç§¯å±‚ä¸­é—´æ’å…¥BNå±‚æ¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œå·ç§¯å±‚é€‰æ‹©ä¸ä½¿ç”¨åç½®å‘é‡ã€‚
'''

class Generator(keras.Model):
    #ç”Ÿæˆå™¨ç½‘ç»œç±»
    def __init__(self):
        super(Generator,self).__init__()
        filter=64
        #è½¬ç½®å·ç§¯å±‚1ï¼Œè¾“å‡ºchannelä¸ºfilter*8,æ ¸å¤§å°4ï¼Œæ­¥é•¿1,ä¸ä½¿ç”¨paddingï¼Œä¸ä½¿ç”¨åç½®
        #å½“è®¾ç½® padding=â€™VALIDâ€™æ—¶ï¼Œè¾“å‡ºå¤§å°è¡¨è¾¾ä¸ºï¼šo=(i-1)s+k
        self.conv1=layers.Conv2DTranspose(filter*8,4,1,'valid',use_bias=False)
        self.bn1=layers.BatchNormalization()
        #è½¬ç½®å·ç§¯å±‚2
        self.conv2=layers.Conv2DTranspose(filter*4,4,2,'same',use_bias=False)
        self.bn2=layers.BatchNormalization()
        #è½¬ç½®å·ç§¯å±‚3
        self.conv3=layers.Conv2DTranspose(filter*2,4,2,'same',use_bias=False)
        self.bn3=layers.BatchNormalization()
        #è½¬ç½®å·ç§¯å±‚4
        self.conv4=layers.Conv2DTranspose(filter*1,4,2,'same',use_bias=False)
        self.bn4=layers.BatchNormalization()
        #è½¬ç½®å·ç§¯5
        self.conv5=layers.Conv2DTranspose(3,4,2,'same',use_bias=False)
        
    def call(self,inputs,training=None):
        x=inputs #[z,100]
        #Reshapeä¸º[b,1,1,100]
        x=tf.reshape(x,(x.shape[0],1,1,x.shape[1]))
        x=tf.nn.relu(x)
        #è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°[b,4,4,512]
        x=tf.nn.relu(self.bn1(self.conv1(x),training=training))
        #è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°[b,8,8,256]
        x=tf.nn.relu(self.bn2(self.conv2(x),training=training))
        #è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°[b,16,16,128]
        x=tf.nn.relu(self.bn3(self.conv3(x),training=training))
        #è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°[b,32,32,64]
        x=tf.nn.relu(self.bn4(self.conv4(x),training=training))
        #è½¬ç½®å·ç§¯-æ¿€æ´»å‡½æ•°[b,64,64,3]
        x=self.conv5(x)
        x=tf.tanh(x)#è¾“å‡ºxèŒƒå›´ä¸º-1~1ï¼Œä¸é¢„å¤„ç†ä¸€è‡´
        
        return x
    
#%%
'''
åˆ¤åˆ«å™¨

åˆ¤åˆ«ç½‘ç»œDä¸æ™®é€šçš„åˆ†ç±»ç½‘ç»œç›¸åŒï¼Œ
æ¥å—å¤§å°ä¸º[ğ‘,64,64,3]çš„å›¾ç‰‡å¼ é‡ï¼Œ
è¿ç»­é€šè¿‡5ä¸ªå·ç§¯å±‚å®ç°ç‰¹å¾çš„å±‚å±‚æå–ï¼Œ
å·ç§¯å±‚æœ€ç»ˆè¾“å‡ºå¤§å°ä¸º[ğ‘, 2,2,1024]ï¼Œ
å†é€šè¿‡æ± åŒ–å±‚ GlobalAveragePooling2Då°†ç‰¹å¾å¤§å°è½¬æ¢ä¸º[ğ‘, 1024]ï¼Œ
æœ€åé€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚è·å¾—äºŒåˆ†ç±»ä»» åŠ¡çš„æ¦‚ç‡ã€‚åˆ¤åˆ«ç½‘ç»œDç±»çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š
'''

class Discriminator(keras.Model):
    #åˆ¤åˆ«å™¨ç±»
    def __init__(self):
        super(Discriminator,self).__init__()
        filter=64
        #å·ç§¯å±‚1
        self.conv1=layers.Conv2D(filter,4,2,'valid',use_bias=False)
        self.bn1=layers.BatchNormalization()
        #å·ç§¯å±‚2
        self.conv2=layers.Conv2D(filter*2,4,2,'valid',use_bias=False)
        self.bn2=layers.BatchNormalization()
        #å·ç§¯å±‚3
        self.conv3=layers.Conv2D(filter*4,4,2,'valid',use_bias=False)
        self.bn3=layers.BatchNormalization()
        #å·ç§¯å±‚4
        self.conv4=layers.Conv2D(filter*8,3,1,'valid',use_bias=False)
        self.bn4=layers.BatchNormalization()
        #å·ç§¯å±‚5
        self.conv5=layers.Conv2D(filter*16,3,1,'valid',use_bias=False)
        self.bn5=layers.BatchNormalization()
        #å…¨å±€æ± åŒ–å±‚
        self.pool=layers.GlobalAveragePooling2D()
        #ç‰¹å¾æ‰“å¹³å±‚
        self.flatten=layers.Flatten()
        #äºŒåˆ†ç±»å…¨è¿æ¥å±‚
        self.fc=layers.Dense(1)
        
    def call(self,inputs,training=None):
        #å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4,31,31,64)
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 31, 31, 64)
        
        x = tf.nn.leaky_relu(self.bn1(self.conv1(inputs), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 14, 14, 128)
        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 6, 6, 256)
        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 4, 4, 512)
        x = tf.nn.leaky_relu(self.bn4(self.conv4(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 2, 2, 1024)
        x = tf.nn.leaky_relu(self.bn5(self.conv5(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 1024)
        x = self.pool(x)
        # æ‰“å¹³
        x = self.flatten(x)
        # è¾“å‡ºï¼Œ[b, 1024] => [b, 1]
        logits = self.fc(x)

        return logits      
    
        #åˆ¤åˆ«å™¨çš„è¾“å‡ºå¤§å°ä¸º[ğ‘, 1]ï¼Œç±»å†…éƒ¨æ²¡æœ‰ä½¿ç”¨ Sigmoid æ¿€æ´»å‡½æ•°ï¼Œ
        #é€šè¿‡ Sigmoid æ¿€æ´»å‡½æ•°å å¯è·å¾—ğ‘ä¸ªæ ·æœ¬å±äºçœŸå®æ ·æœ¬çš„æ¦‚ç‡
    
#%%

'''
è®­ç»ƒä¸å¯è§†åŒ–

'''

#%%

'''
åˆ¤åˆ«ç½‘ç»œ

åˆ¤åˆ«ç½‘ç»œçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–â„’(ğ·, ğº)å‡½æ•°ï¼Œ
ä½¿å¾—çœŸå®æ ·æœ¬é¢„æµ‹ä¸ºçœŸçš„æ¦‚ç‡æ¥è¿‘äº 1ï¼Œç”Ÿæˆæ ·æœ¬é¢„æµ‹ä¸ºçœŸçš„æ¦‚ç‡æ¥è¿‘äº 0ã€‚
æˆ‘ä»¬å°†åˆ¤æ–­å™¨çš„è¯¯å·®å‡½æ•°å®ç° åœ¨ d_loss_fn å‡½æ•°ä¸­ï¼Œ
å°†æ‰€æœ‰çœŸå®æ ·æœ¬æ ‡æ³¨ä¸º 1ï¼Œæ‰€æœ‰ç”Ÿæˆæ ·æœ¬æ ‡æ³¨ä¸º 0ï¼Œ
å¹¶é€šè¿‡æœ€å°åŒ–å¯¹åº”çš„äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å®ç°æœ€å¤§åŒ–â„’(ğ·, ğº)å‡½æ•°ã€‚
'''

def celoss_zeros(logits):
    # è®¡ç®—å±äºä¸ä¾¿ç­¾ä¸º0çš„äº¤å‰ç†µ
    y = tf.zeros_like(logits)
    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)
    return tf.reduce_mean(loss)

def celoss_ones(logits):
    # è®¡ç®—å±äºä¸æ ‡ç­¾ä¸º1çš„äº¤å‰ç†µ
    y = tf.ones_like(logits)
    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)
    return tf.reduce_mean(loss)

def d_loss_fn(generator,discriminator,batch_z,batch_x,is_training):
    #è®¡ç®—åˆ¤åˆ«å™¨çš„è¯¯å·®å‡½æ•°
    #é‡‡æ ·ç”Ÿæˆå›¾ç‰‡
    fake_image=generator(batch_z,is_training)
    #åˆ¤å®šç”Ÿæˆå›¾ç‰‡
    d_fake_logits=discriminator(fake_image,is_training)
    #åˆ¤å®šçœŸå®å›¾ç‰‡
    d_real_logits=discriminator(batch_x,is_training)
    #çœŸå®å›¾ç‰‡ä¸1ä¹‹é—´çš„è¯¯å·®
    d_loss_real=celoss_ones(d_real_logits)
    #ç”Ÿæˆå›¾ç‰‡ä¸0ä¹‹é—´çš„è¯¯å·®
    d_loss_fake=celoss_zeros(d_fake_logits)
    #åˆå¹¶è¯¯å·®
    loss=d_loss_fake+d_loss_real
    
    return loss

#%%
'''
ç”Ÿæˆç½‘ç»œ

ç”±äºçœŸå®æ ·æœ¬ä¸ç”Ÿæˆå™¨æ— å…³ï¼Œ
å› æ­¤è¯¯å·®å‡½æ•°åªéœ€è¦è€ƒè™‘æœ€å°åŒ–ğ”¼ğ’›~ğ‘ğ‘§(âˆ™)log(1 âˆ’ ğ·ğœƒ(ğºğœ™(ğ’›)))é¡¹å³å¯ã€‚
å¯ä»¥é€šè¿‡å°†ç”Ÿæˆçš„æ ·æœ¬æ ‡ æ³¨ä¸º 1ï¼Œæœ€å°åŒ–æ­¤æ—¶çš„äº¤å‰ç†µè¯¯å·®ã€‚
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨åå‘ä¼ æ’­è¯¯å·®çš„è¿‡ç¨‹ä¸­ï¼Œåˆ¤åˆ«å™¨ä¹Ÿå‚ä¸äº†è®¡ç®—å›¾çš„æ„å»ºï¼Œ
ä½†æ˜¯æ­¤é˜¶æ®µåªéœ€è¦æ›´æ–°ç”Ÿæˆå™¨ç½‘ç»œå‚æ•°ï¼Œè€Œä¸æ›´æ–°åˆ¤åˆ«å™¨çš„ç½‘ç»œå‚æ•°ã€‚

'''

def g_loss_fn(generator,discriminator,batch_z,is_training):
    #é‡‡æ ·ç”Ÿæˆå›¾ç‰‡
    fake_image=generator(batch_z,is_training)
    #è®­ç»ƒç”Ÿæˆç½‘ç»œæ—¶ï¼Œéœ€è¦è¿«ä½¿ç”Ÿæˆå›¾ç‰‡åˆ¤å®šä¸ºçœŸ
    d_fake_logits=discriminator(fake_image,is_training)
    #è®¡ç®—ç”Ÿæˆå›¾ç‰‡ä¸1ä¹‹é—´çš„è¯¯å·®
    loss=celoss_ones(d_fake_logits)
    
    return loss

#%%

'''
ç½‘ç»œè®­ç»ƒ

åœ¨æ¯ä¸ª Epochï¼Œé¦–å…ˆä»å…ˆéªŒåˆ†å¸ƒğ‘ (âˆ™)ä¸­éšæœºé‡‡æ ·éšè—å‘é‡ï¼Œ
ä»çœŸå®æ•°æ®é›†ä¸­éšæœºé‡‡æ ·çœŸå®å›¾ç‰‡ï¼Œ
é€šè¿‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨è®¡ç®—åˆ¤åˆ«å™¨ç½‘ç»œçš„æŸå¤±ï¼Œ
å¹¶ä¼˜åŒ–åˆ¤åˆ«å™¨ç½‘ç»œå‚æ•°ğœƒã€‚
åœ¨è®­ç»ƒç”Ÿæˆå™¨æ—¶ï¼Œéœ€è¦å€ŸåŠ©äºåˆ¤åˆ«å™¨æ¥è®¡ç®—è¯¯å·®ï¼Œä½†æ˜¯åªè®¡ç®—ç”Ÿæˆå™¨çš„æ¢¯åº¦ä¿¡æ¯å¹¶æ›´æ–°ğœ™ã€‚
è¿™é‡Œè®¾å®šåˆ¤åˆ«å™¨è®­ç»ƒğ‘˜ = 5æ¬¡åï¼Œç”Ÿæˆå™¨è®­ç»ƒä¸€æ¬¡ã€‚
'''

#é¦–å…ˆåˆ›å»ºç”Ÿæˆç½‘ç»œå’Œåˆ¤åˆ«ç½‘ç»œï¼Œå¹¶åˆ†åˆ«åˆ›å»ºå¯¹åº”çš„ä¼˜åŒ–å™¨

def main():
    z_dim=100#éšè—å˜é‡zçš„ç»´åº¦
    
    generator=Generator()#åˆ›å»ºç”Ÿæˆå™¨
    generator.build(input_shape=(batch_size,z_dim))
    generator.summary()
    discriminator=Discriminator()#åˆ›å»ºåˆ¤åˆ«å™¨
    discriminator.build(input_shape=(None,64,64,3))
    discriminator.summary()
    #åˆ†åˆ«ä¸ºç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨åˆ›å»ºä¼˜åŒ–å™¨
    
    if os.path.exists('checkpoint')==True:
        generator.load_weights('generator.ckpt')
        print('Loaded generator.ckpt!')
        discriminator.load_weights('discriminator.ckpt')
        print('Loaded discriminator.ckpt!')

    learning_rate=0.0002
    g_optimizer=keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.5)
    d_optimizer=keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.5)
    
    epochs=100000#ä¼šä¸€å®šé—´éš”åä¿å­˜æƒé‡ï¼Œæ‰€ä»¥epochå¯ä»¥æ— é™
    db_iter = iter(dataset)
    
    is_training=True
        
    #ä¸»è®­ç»ƒéƒ¨åˆ†ä»£ç å®ç°å¦‚ä¸‹
    for epoch in range(epochs):#è®­ç»ƒepochsæ¬¡
        #1.è®­ç»ƒåˆ¤åˆ«å™¨
        batch_z=tf.random.normal([batch_size,z_dim])
        batch_x=next(db_iter)#é‡‡ç”¨çœŸå®ç…§ç‰‡
        #åˆ¤åˆ«å™¨å‰å‘è®¡ç®—
        with tf.GradientTape() as tape:
            d_loss=d_loss_fn(generator,discriminator,batch_z,batch_x,is_training)
        grads=tape.gradient(d_loss,discriminator.trainable_variables)
        d_optimizer.apply_gradients(zip(grads,discriminator.trainable_variables))
        #2.è®­ç»ƒç”Ÿæˆå™¨
        #é‡‡æ ·éšè—å˜é‡
        for _ in range(2):
	        batch_z=tf.random.normal([batch_size,z_dim])
	        with tf.GradientTape() as tape:
	            g_loss=g_loss_fn(generator,discriminator,batch_z,is_training)
	        grads=tape.gradient(g_loss,generator.trainable_variables)
	        g_optimizer.apply_gradients(zip(grads,generator.trainable_variables))
        
        #æ¯é—´éš”100ä¸ªEpochï¼Œè¿›è¡Œä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆæµ‹è¯•ã€‚
        #é€šè¿‡ä»å…ˆéªŒåˆ†å¸ƒä¸­éšæœºé‡‡æ ·éšå‘é‡ï¼Œé€å…¥ ç”Ÿæˆå™¨è·å¾—ç”Ÿæˆå›¾ç‰‡ï¼Œå¹¶ä¿å­˜ä¸ºæ–‡ä»¶ã€‚
        
        if epoch % 100 == 0:
            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss))
            # å¯è§†åŒ–
            z = tf.random.normal([100, z_dim])
            fake_image = generator(z, training=False)
            img_path = os.path.join('gan_images', 'gan-%d.png'%epoch)
            save_result(fake_image.numpy(), 10, img_path, color_mode='P')
    
            if epoch>0 and epoch % 1000 == 0:
                generator.save_weights('generator.ckpt')
                discriminator.save_weights('discriminator.ckpt')
                
                if epoch%5000==0:
                    learning_rate/=2
                    g_optimizer=keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.5)
                    d_optimizer=keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.5)

def save_result(val_out, val_block_size, image_path, color_mode):
    '''
    Parameters
    ----------
    val_out : [100,64,64,3]çš„ç”Ÿæˆå›¾ç‰‡
    val_block_size : æ¯è¡Œæ¯åˆ—val_block_sizeä¸ªå›¾ç‰‡
    image_path : å›¾ç‰‡å­˜å‚¨è·¯å¾„
    color_mode : TYPE
    '''
    def preprocess(img):
        img = ((img + 1.0) * 127.5).astype(np.uint8)
        return img

    preprocesed = preprocess(val_out)
    final_image = np.array([])
    single_row = np.array([])
    for b in range(val_out.shape[0]):
        # concat image into a row
        if single_row.size == 0:
            single_row = preprocesed[b, :, :, :]
        else:
            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)
            #åœ¨[h,w,c]çš„wç»´åº¦æ’å…¥

        # concat image row to final_image
        if (b+1) % val_block_size == 0:
            if final_image.size == 0:
                final_image = single_row
            else:
                final_image = np.concatenate((final_image, single_row), axis=0)
                #åœ¨[h,w,c]çš„hç»´åº¦æ’å…¥
            # reset single row
            single_row = np.array([])

    if final_image.shape[2] == 1:
        #å¦‚æœcé€šé“ä¸º1ï¼Œå°±ä¿ç•™1
        final_image = np.squeeze(final_image, axis=2)
    Image.fromarray(final_image).save(image_path)

    
#%%
'''
ç”Ÿæˆå•å¼ å›¾ç‰‡
'''
import random

def product():
    def preprocess(img):
        img = ((img + 1.0) * 127.5).astype(np.uint8)
        return img
    
    z_dim=100
    Input=tf.random.normal([1,z_dim])
    generator=Generator()#åˆ›å»ºç”Ÿæˆå™¨
    generator.load_weights('generator.ckpt')
    print('Loaded generator.ckpt!')
    Output=generator(Input,training=False)
    del generator
    image_path=str(random.randint(0,int(1e9)))+'.png'
    Image.fromarray(np.array(preprocess(Output[0,:,:,:].numpy()))).save(image_path)
    
#%%
    
if __name__=='__main__':
    main()
    
    
