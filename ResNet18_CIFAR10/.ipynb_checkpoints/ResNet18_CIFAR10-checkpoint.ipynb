{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T14:45:56.978556Z",
     "start_time": "2020-02-19T14:45:56.973570Z"
    }
   },
   "source": [
    "# CIFAR10andResNet18\n",
    "\n",
    "## ResNetåŸç†\n",
    "\n",
    "ResNet é€šè¿‡åœ¨å·ç§¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºä¹‹é—´æ·»åŠ Skip Connection å®ç°å±‚æ•°å›é€€æœºåˆ¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¾“å…¥$x$é€šè¿‡ä¸¤ä¸ªå·ç§¯å±‚ï¼Œå¾—åˆ°ç‰¹å¾è¡¨æ¢åçš„è¾“å‡º$F(x)$ï¼Œä¸è¾“å…¥$x$è¿›è¡Œå¯¹åº”å…ƒç´ çš„ç›¸åŠ è¿ç®—ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡º$H(x)$:\n",
    "\n",
    "$$H(x)=x+F(x)$$\n",
    "\n",
    "$H(x)$å«ä½œæ®‹å·®æ¨¡å—(Residual Blockï¼Œç®€ç§°ResBlock)ã€‚ç”±äºè¢« Skip Connection åŒ…å›´çš„å·ç§¯ç¥ ç»ç½‘ç»œéœ€è¦å­¦ä¹ æ˜ å°„$F(x)=H(x)-x$ï¼Œæ•…ç§°ä¸ºæ®‹å·®ç½‘ç»œã€‚\n",
    "\n",
    "<img src=\"ResBlock.PNG\" width=\"40%\">\n",
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿæ»¡è¶³è¾“å…¥$x$ä¸å·ç§¯å±‚çš„è¾“å‡º$F(x)$èƒ½å¤Ÿç›¸åŠ è¿ç®—ï¼Œéœ€è¦è¾“å…¥$x$çš„shapeä¸$F(x)$çš„shapeå®Œå…¨ä¸€è‡´ã€‚å½“å‡ºç°shapeä¸ä¸€è‡´æ—¶ï¼Œä¸€èˆ¬é€šè¿‡åœ¨ Skip Connection ä¸Šæ·»åŠ é¢å¤–çš„å·ç§¯è¿ç®—ç¯èŠ‚å°†è¾“å…¥$x$å˜æ¢åˆ°ä¸$F(x)$ç›¸åŒçš„shapeï¼Œå¦‚å›¾ 10.63 ä¸­$identity(x)$å‡½æ•°æ‰€ç¤ºï¼Œå…¶ä¸­$identity(x)$ä»¥ $1*1$çš„å·ç§¯è¿ç®—å±…å¤šï¼Œä¸»è¦ç”¨äºè°ƒæ•´è¾“å…¥çš„é€šé“æ•°ã€‚\n",
    "\n",
    "è¿™é‡Œçš„ç›¸åŠ è®¡ç®—æŒ‡ï¼š\n",
    "\n",
    "ä¸¤ä¸ª\\[n,h,r,c\\]çš„ç½‘ç»œç›¸åŠ å¾—åˆ°ä¸€ä¸ª\\[n,h,r,c\\]çš„ç½‘ç»œã€‚\n",
    "\n",
    "## ResNet18\n",
    "\n",
    "æœ¬é¡µå°†å®ç°18å±‚çš„**æ·±åº¦æ®‹å·®ç½‘ç»œ**ResNet18ï¼Œå¹¶åœ¨CIFAR10å›¾ç‰‡é›†ä¸Šè®­ç»ƒä¸æµ‹è¯•ã€‚\n",
    "\n",
    "æ ‡å‡†çš„ResNet18æ¥å—è¾“å…¥ä¸º224*224å¤§å°çš„å›¾ç‰‡æ•°æ®ï¼Œæˆ‘ä»¬å°†ResNet18 è¿›è¡Œé€‚é‡è°ƒæ•´ï¼Œä½¿å¾—å®ƒè¾“å…¥å¤§å°ä¸º32 Ã— 32ï¼Œè¾“å‡ºç»´åº¦ä¸º10ã€‚è°ƒæ•´åçš„ResNet18ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾ã€‚\n",
    "\n",
    "<img src=\"ResNet.PNG\">\n",
    "\n",
    "## å®ç°\n",
    "\n",
    "é¦–å…ˆå®ç°ä¸­é—´ä¸¤ä¸ªå·ç§¯å±‚ï¼ŒSkip Connection 1\\*1å·ç§¯å±‚çš„æ®‹å·®æ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:50:58.431398Z",
     "start_time": "2020-02-21T03:50:52.111794Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Sequential,optimizers,losses,datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "    #æ®‹å·®æ¨¡å—\n",
    "    def __init__(self,filter_num,stride=1):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        #ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ\n",
    "        self.conv1=layers.Conv2D(filter_num,(3,3),strides=stride,padding='SAME')\n",
    "        self.bn1=layers.BatchNormalization()\n",
    "        self.relu=layers.Activation('relu')\n",
    "        #ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ\n",
    "        self.conv2=layers.Conv2D(filter_num,(3,3),strides=1,padding='SAME')\n",
    "        self.bn2=layers.BatchNormalization()\n",
    "        \n",
    "        if stride != 1:#é€šè¿‡1*1å·ç§¯å®ŒæˆshapeåŒ¹é…\n",
    "            self.downsample=Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num,(1,1),strides=stride))\n",
    "        else:#shapeåŒ¹é…ï¼Œç›´æ¥çŸ­æ¥\n",
    "            self.downsample=lambda x:x\n",
    "        \n",
    "    def call(self,inputs,training=None):\n",
    "        #å‰å‘è®¡ç®—å‡½æ•°\n",
    "        #[b,h,w,c]ï¼Œé€šè¿‡ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ\n",
    "        out=self.conv1(inputs)\n",
    "        out=self.bn1(out)\n",
    "        out=self.relu(out)\n",
    "        #é€šè¿‡ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ\n",
    "        out=self.conv2(out)\n",
    "        out=self.bn2(out)\n",
    "        #é€šè¿‡identityæ¨¡å—\n",
    "        identity=self.downsample(inputs)\n",
    "        #2æ¡è·¯å¾„è¾“å‡ºç›´æ¥ç›¸åŠ \n",
    "        output=layers.add([out,identity])\n",
    "        output=tf.nn.relu(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è®¾è®¡æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ—¶ï¼Œä¸€èˆ¬æŒ‰ç…§ç‰¹å¾å›¾é«˜å®½â„/ğ‘¤é€æ¸å‡å°‘ï¼Œé€šé“æ•°ğ‘é€æ¸å¢å¤§çš„ç»éªŒæ³•åˆ™ã€‚å¯ä»¥é€šè¿‡å †å é€šé“æ•°é€æ¸å¢å¤§çš„ResBlockæ¥å®ç°é«˜å±‚ç‰¹å¾çš„æå–ï¼Œé€šè¿‡build_resblockå¯ä»¥ä¸€æ¬¡å®Œæˆå¤šä¸ªæ®‹å·®æ¨¡å—çš„æ–°å»ºã€‚\n",
    "\n",
    "ä¸‹é¢å®ç°é€šç”¨çš„ResNet ç½‘ç»œæ¨¡å‹\n",
    "\n",
    "è¡¥å……ï¼š\n",
    "\n",
    "help(layers.GlobalAveragePooling2D)\n",
    "\n",
    "    GlobalAveragePooling2D(data_format=None, **kwargs)\n",
    "    |  \n",
    "    |  Global average pooling operation for spatial data.\n",
    "    |  \n",
    "    |  Arguments:\n",
    "    |      data_format: A string,\n",
    "    |  \n",
    "    |  Input shape:\n",
    "    |    - If `data_format='channels_last'`:(default)\n",
    "    |      4D tensor with shape `(batch_size, rows, cols, channels)`.\n",
    "    |    - If `data_format='channels_first'`:\n",
    "    |      4D tensor with shape `(batch_size, channels, rows, cols)`.\n",
    "    |  \n",
    "    |  Output shape:\n",
    "    |    2D tensor with shape `(batch_size, channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:01.982433Z",
     "start_time": "2020-02-21T03:51:01.967472Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(keras.Model):\n",
    "    #é€šç”¨çš„ResNetå®ç°ç±»\n",
    "    \n",
    "    def __init__(self,layer_dims,num_classes=10):#[2,2,2,2]\n",
    "        super(ResNet,self).__init__()\n",
    "        #æ ¹ç½‘ç»œï¼Œé¢„å¤„ç†\n",
    "        self.stem=Sequential([\n",
    "            layers.Conv2D(64,(3,3),strides=(1,1)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding='SAME')\n",
    "            ])\n",
    "        \n",
    "        #å †å 4ä¸ªBlockï¼Œæ¯ä¸ªBlockåŒ…å«äº†å¤šä¸ªBasicBlockï¼Œè®¾ç½®æ­¥é•¿ä¸ä¸€æ ·\n",
    "        self.layer1=self.build_resblock(64,layer_dims[0])\n",
    "        self.layer2=self.build_resblock(128,layer_dims[1],stride=2)\n",
    "        self.layer3=self.build_resblock(256,layer_dims[2],stride=2)\n",
    "        self.layer4=self.build_resblock(512,layer_dims[3],stride=2)\n",
    "        \n",
    "        #é€šè¿‡Poolingå±‚å°†é«˜å®½é™ä½ä¸º1*1\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        #æœ€åè¿æ¥æˆä¸€ä¸ªå…¨è¿æ¥å±‚åˆ†ç±»\n",
    "        self.fc=layers.Dense(num_classes)\n",
    "        \n",
    "    def call(self,inputs,training=None):\n",
    "        #å‰å‘è®¡ç®—å‡½æ•°ï¼šé€šè¿‡æ ¹ç½‘ç»œ\n",
    "        x=self.stem(inputs)\n",
    "        #ä¸€æ¬¡é€šè¿‡4ä¸ªæ¨¡å—\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        #é€šè¿‡æ± åŒ–å±‚\n",
    "        x=self.avgpool(x)\n",
    "        #é€šè¿‡å…¨è¿æ¥å±‚\n",
    "        x=self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build_resblock(self,filter_num,blocks,stride=1):\n",
    "        #è¾…åŠ©å‡½æ•°ï¼Œå †å filter_numä¸ªBasicBlock\n",
    "        res_blocks=Sequential()\n",
    "        #åªæœ‰ç¬¬ä¸€ä¸ª BasicBlock çš„æ­¥é•¿å¯èƒ½ä¸ä¸º1ï¼Œå®ç°ä¸‹é‡‡æ ·\n",
    "        res_blocks.add(BasicBlock(filter_num,stride))\n",
    "        \n",
    "        for _ in range(1,blocks):#å…¶ä»–BasicBlockæ­¥é•¿éƒ½ä¸º1\n",
    "            res_blocks.add(BasicBlock(filter_num,stride=1))\n",
    "    \n",
    "        return res_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡è°ƒæ•´æ¯ä¸ªRes Blockçš„å †å æ•°é‡å’Œé€šé“æ•°å¯ä»¥äº§ç”Ÿä¸åŒçš„ResNetï¼Œå¦‚é€šè¿‡64-64-128-128-256-256-512-512é€šé“æ•°é…ç½®ï¼Œå…±8ä¸ªResBlockï¼Œå¯å¾—åˆ°ResNet18çš„ç½‘ç»œæ¨¡å‹ã€‚æ¯ä¸ª ResBlock åŒ…å«äº†2ä¸ªä¸»è¦çš„å·ç§¯å±‚ï¼Œå› æ­¤å·ç§¯å±‚æ•°é‡æ˜¯8âˆ™2=16ï¼ŒåŠ ä¸Šç½‘ç»œæœ«å°¾çš„å…¨è¿æ¥å±‚ï¼Œå…±18å±‚ã€‚åˆ›å»ºResNet18å’ŒResNet34å¯ä»¥ç®€å•å®ç°å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:03.861797Z",
     "start_time": "2020-02-21T03:51:03.855814Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet18():\n",
    "    return ResNet([2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet([3,4,6,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒå¹¶ä¿å­˜ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:15:02.875774Z",
     "start_time": "2020-02-21T02:18:00.041399Z"
    }
   },
   "outputs": [],
   "source": [
    "model=resnet18()\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary()\n",
    "optimizer = optimizers.Adam(lr=1e-4) # æ„å»ºä¼˜åŒ–å™¨\n",
    "\n",
    "#%%\n",
    "(x,y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "#åˆ é™¤yçš„ä¸€ä¸ªç»´åº¦,[b,1]=>[b]\n",
    "y=tf.squeeze(y,axis=1)\n",
    "y_test=tf.squeeze(y_test,axis=1)\n",
    "#æ‰“å°è®­ç»ƒæ¥å’Œæµ‹è¯•é›†çš„å½¢çŠ¶\n",
    "print(x.shape,y.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # å°†æ•°æ®æ˜ å°„åˆ°-1~1\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32) # ç±»å‹è½¬æ¢\n",
    "    return x,y\n",
    "\n",
    "#æ„å»ºè®­ç»ƒé›†å¯¹è±¡ï¼Œéšæœºæ‰“ä¹±ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–\n",
    "train_db=tf.data.Dataset.from_tensor_slices((x,y))\n",
    "train_db=train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "#æ„å»ºæµ‹è¯•é›†å¯¹è±¡ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–\n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "test_db=test_db.map(preprocess).batch(128)\n",
    "#ä»è®­ç»ƒé›†ä¸­é‡‡ç”¨ä¸€ä¸ªBatchï¼Œå¹¶è§‚å¯Ÿ\n",
    "sample=next(iter(train_db))\n",
    "print('sample:',sample[0].shape,sample[1].shape,tf.reduce_min(sample[0]),tf.reduce_max(sample[0]))\n",
    "\n",
    "#%%\n",
    "#æ¨¡å‹è®¡ç®—\n",
    "def main():\n",
    "    for epoch in range(5):\n",
    "        for step,(x,y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                #[b,32,32,3] => [b,10]\n",
    "                logits=model(x)\n",
    "                #[b] => [b,10]\n",
    "                y_onehot=tf.one_hot(y,depth=10)\n",
    "                #compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot,logits,from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            grads = tape.gradient(loss,model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "            if step%100==0:\n",
    "                print(epoch,step,'loss:',float(loss))\n",
    "                \n",
    "        total_num=0\n",
    "        total_corret=0\n",
    "        for x,y in test_db:\n",
    "            logits=model(x)\n",
    "            prob=tf.nn.softmax(logits,axis=1)\n",
    "            pred=tf.argmax(prob,axis=1)\n",
    "            pred=tf.cast(pred,dtype=tf.int32)\n",
    "            \n",
    "            correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
    "            correct=tf.reduce_sum(correct)\n",
    "            \n",
    "            total_num+=x.shape[0]\n",
    "            total_corret+=int(correct)\n",
    "        \n",
    "        acc=total_corret/total_num\n",
    "        print(epoch,'acc:',acc)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.save('ResNet18.h5',save_format=\"tf\") \n",
    "print('saving ResNet18')\n",
    "```\n",
    "ä¼šæŠ¥é”™ï¼š\n",
    "\n",
    "    NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
    "\n",
    "å› ä¸ºè‡ªå®šä¹‰ç½‘ç»œä¸èƒ½ç›´æ¥ä¿å­˜æ•´ä¸ªç½‘ç»œï¼Œè€Œå¯ä»¥é‡‡ç”¨ä¿å­˜æƒå€¼çš„æ–¹å¼ã€‚\n",
    "\n",
    "è¿™ç§ä¿å­˜ä¸åŠ è½½ç½‘ç»œçš„æ–¹å¼æœ€ä¸ºè½»é‡çº§ï¼Œæ–‡ä»¶ä¸­ä¿å­˜çš„ä»…ä»…æ˜¯å¼ é‡å‚æ•°çš„æ•°å€¼ï¼Œå¹¶æ²¡æœ‰å…¶ å®ƒé¢å¤–çš„ç»“æ„å‚æ•°ã€‚ä½†æ˜¯å®ƒéœ€è¦ä½¿ç”¨ç›¸åŒçš„ç½‘ç»œç»“æ„æ‰èƒ½å¤Ÿæ­£ç¡®æ¢å¤ç½‘ç»œçŠ¶æ€ï¼Œå› æ­¤ä¸€èˆ¬ åœ¨æ‹¥æœ‰ç½‘ç»œæºæ–‡ä»¶çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:45:13.728718Z",
     "start_time": "2020-02-21T03:45:13.359672Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights('ResNet18_weights.ckpt') \n",
    "print('saved weights.')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:13.486583Z",
     "start_time": "2020-02-21T03:51:10.640442Z"
    }
   },
   "outputs": [],
   "source": [
    "model=resnet18()\n",
    "model.load_weights('ResNet18_weights.ckpt') \n",
    "print('loaded weights!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¾“å‡ºå›¾ç‰‡è¯†åˆ«ç»“æœ\n",
    "\n",
    "ä»¥ä¸‹éƒ¨åˆ†åŒï¼šCIFAR10andVGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:13.495560Z",
     "start_time": "2020-02-21T03:51:13.488578Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer=dict({0:\"airplane\",\n",
    "        1:\"automobile\",\n",
    "        2:\"bird\",\n",
    "        3:\"cat\",\n",
    "        4:\"deer\",\n",
    "        5:\"dog\",\n",
    "        6:\"frog\",\n",
    "        7:\"horse\",\n",
    "        8:\"ship\",\n",
    "        9:\"truck\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:13.586327Z",
     "start_time": "2020-02-21T03:51:13.500547Z"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def test(file_name):\n",
    "    image_value=imageio.imread(file_name)[::,::,0:3]#å› ä¸ºè¯»è¿›æ¥æ—¶æ˜¯å››é€šé“\n",
    "    plt.imshow(image_value)\n",
    "    image_value=tf.image.resize(image_value,[32,32],antialias=True)\n",
    "    image_value=tf.expand_dims(image_value,axis=0)\n",
    "    image_value=2*tf.cast(image_value, dtype=tf.float32) / 255. - 1\n",
    "    logits=model(image_value)\n",
    "    res=tf.argmax(logits,axis=1)\n",
    "    plt.xlabel(transfer[int(res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T03:51:19.395143Z",
     "start_time": "2020-02-21T03:51:13.956330Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name=\"cat.jpg\"\n",
    "test(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
